#+TITLE: The Algorithm Design Manual
#+AUTHOR: Elliot Penson

This document houses my notes from /The Algorithm Design Manual/ by Steven
S. Skiena. Also see [[file:algorithm-design-manual-exercises.org][my answers to odd exercises]].

* Schedule

  | Week | Start Date   | Chapter                                                    |   Pages | Done? |
  |------+--------------+------------------------------------------------------------+---------+-------|
  |    1 | September 16 | Introduction to Algorithm Design                           |    1-30 | X     |
  |    2 | September 23 | Algorithm Analysis                                         |   31-64 | X     |
  |    3 | September 30 | Data Structures (Part 1)                                   |   65-83 | X     |
  |    4 | October 7    | Data Structures (Part 2)                                   |  83-102 |       |
  |    5 | October 14   | Sorting and Searching                                      | 103-144 |       |
  |    6 | October 21   | Graph Traversal (Part 1)                                   | 145-169 |       |
  |    7 | October 28   | Graph Traversal (Part 2)                                   | 169-190 |       |
  |    8 | November 4   | Weighted Graph Algorithms                                  | 191-229 |       |
  |    9 | November 11  | Combinatorial Search and Heuristic Methods                 | 230-272 |       |
  |   10 | November 18  | Dynamic Programming                                        | 273-315 |       |
  |   11 | November 25  | Intractable Problems and Approximation Algorithms (Part 1) | 316-330 |       |
  |   12 | December 2   | Intractable Problems and Approximation Algorithms (Part 1) | 330-355 |       |

* Chapter 1: Introduction

  A *problem* is specified by describing the complete set of *instances* it must
  work on and of its output after running on one of these instances. For
  example, an instance of the sorting problem might be ~[1, 4, 3]~. An
  *algorithm* is a procedure that takes any of the possible input instances and
  transforms it to the desired output. There are many different algorithms that
  solve the problem of sorting.

  We seek algorithms that are /correct/, /efficient/, and /easy to
  implement/. These goals may not be simultaneously achievable!

  There is a fundamental difference between algorithms, which always produce a
  correct result, and *heuristics*, which may usually do a good job but without
  providing any guarantee. For example, the nearest-neighbor heuristic can be
  applied to the /traveling salesman problem (TSP)/, but sometimes this
  heuristic doesn't even come close to finding the shortest possible tour.

** Correctness

   Reasonable-looking algorithms can easily be incorrect. Algorithm correctness
   is a property that must be carefully demonstrated. The primary tool to
   distinguish correct algorithms from incorrect ones is a *proof*. A proof has
   four main parts.

   1. Clear, precise statement of what you are trying to prove.
   2. Set of assumptions.
   3. Chain of reasoning that takes you from the assumptions to the statement
      you are trying to prove.
   4. Little, black square or /QED/.

   To reason about an algorithm, you need a careful description of the sequence
   of steps to be performed. The three most common forms of algorithmic notation
   are English, pseudocode, or a real programming language. These methods have
   natural tradeoffs between expression and precision.

*** Incorrectness

   The best way to prove that an algorithm is /incorrect/ is to produce an
   instance in which it yields an incorrect answer. Such instances are called
   *counter-examples*. Good counter-examples are verifiable and simple. Many
   tricks exist for finding counter-examples.

   - /Think small/: small examples are easier to reason about.
   - /Think exhaustively/: consider different types of examples.
   - /Go for a tie/: try similar values in the input collection.
   - /Seek extremes/: e.g. use values that are far apart or close together.

*** Induction

    *Mathematical induction* is a common choice for proving correctness. If
    you're familiar with recursion, you're familiar with induction; recursion
    /is/ induction. In both, we have general and boundary conditions. The
    general condition breaks the problem into smaller and smaller pieces and the
    boundary condition terminates the recursion. Suppose that you are trying to
    prove that a statement holds true for all natural numbers (all
    $n$). Induction would usually take the form:

    1. Show that the statement holds for the base case (usually $P(0)$ or
       $P(1)$).
    2. Assume that $P(k)$ is true.
    3. Prove that the statement also holds for $P(k + 1)$.

*** Summations

    Summation formula are concise expressions describing the addition of an
    arbitrarily large set of numbers.

    \begin{equation}
    \sum_{i=1}^{n} f(i) = f(1) + f(2) + ... + f(n)
    \end{equation}

    There are simple closed forms for summations of many algebraic functions.

    \begin{equation}
    \sum_{i=1}^{n} 1 = n
    \end{equation}

    \begin{equation}
    \sum_{i=1}^{n} i = \frac{n(n + 1)}{2}
    \end{equation}

** Program Modeling

   Modeling is the art of formulating your application in terms of precisely
   described, well-understood problems. Proper modeling can eliminate the need
   to design or even implement algorithms, by relating your application to what
   has been done before.

   Real-world applications involve real-world objects. Most algorithms, however,
   are designed to work on rigorously defined /abstract/ structures. To exploit
   the algorithms literature, you must learn to describe your problem
   abstractly, in terms of procedures on fundamental structures.

   - *Permutations* are arrangements, or orderings of items. Usually the object
     in question if your problem seeks an "arrangement," "tour," "ordering," or
     "sequence."
   - *Subsets* are selects from a set of items. Usually the object in question
     if your problem seeks a "cluster," "collection," "committee," "group,"
     "packaging," or "selection."
   - *Trees* are hierarchical relationships between items. Usually the object in
     question whenever your problem seeks a "hierarchy," "dominance
     relationship," "ancestor/descendant relationship," or "taxonomy."
   - *Graphs* represent relationships between arbitrary pairs of
     objects. Usually the object in question whenever you seek a "network,"
     "circuit," "web," or "relationship."
   - *Points* represent locations in some geometric space. Usually the object in
     question whenever your problems work on "sites," "positions," "date
     records," or "locations."
   - *Polygons* represent regions in some geometric spaces. Usually the object
     in question whenever you are working on "shapes," "regions,"
     "configurations," or "boundaries."
   - *Strings* represent sequences of characters or patterns. Usually the object
     in question whenever you are dealing with "text," "characters," "patterns,"
     or "labels."

   Learn to think recursively. Recursive structures occur everywhere in the
   algorithmic world. Each of the abstract structures described above can be
   thought about recursively; they are big things made of smaller things of the
   same type. Each structure has operations (like /delete/) that produce new
   versions of the same type.

* Chapter 2: Algorithm Analysis

** RAM Model of Computation

   Machine-independent algorithm design depends upon a hypothetical computer
   called the *Random Access Machine* or *RAM*. Under this model of computation,
   we are confronted with a computer where

   - Each /simple/ operation (+, *, -, =, if, call) takes exactly one time
     step.
   - Loops and subroutines are the composition of many single-step operations.
   - Each memory access takes exactly one time step. Further, we have as much
     memory as we need.

   Under the RAM model, we measure run time by counting up the number of steps
   an algorithm takes on a given problem instance. We consider different time
   complexities that define a numerical function, representing time versus
   problem size.

   - *Worst-case complexity* of the algorithm is the function defined by the
     maximum number of steps taken in any instance of size $n$.
   - *Best-case complexity* of the algorithm is the function defined by the
     minimum number of steps taken in any instance of size $n$.
   - *Average-case complexity* of the algorithm is the function defined by the
     average number of steps taken in any instance of size $n$.

** Big Oh Notation

   *Big Oh* simplifies our analysis by ignoring levels of detail that do not
   impact our comparison of algorithms. The formal definitions are as follows:

   - $f(n) = O(g(n))$ means $c \cdot g(n)$ is an /upper bound/ on $f(n)$. Thus
     there exists some constant $c$ such that $f(n)$ is always $\leq c \cdot
     g(n)$, for large enough $n$.
   - $f(n) = \Omega(g(n))$ means $c \cdot g(n)$ is an /lower bound/ on
     $f(n)$. Thus there exists some constant $c$ such that $f(n)$ is always
     $\geq c \cdot g(n)$, for large enough $n$.
   - $f(n) = \Theta(g(n))$ means $c_1 \cdot g(n)$ is an upper bound on $f(n)$
     and $c_2 \cdot g(n)$ is a lower bound on $f(n)$. Thus there exists
     constants $c_1$ and $c_2$ such that $f(n) \leq c_1 \cdot g(n)$ and $f(n)
     \geq c_2 \cdot g(n)$.

   For example, $2n^2 + 100n + 6 = O(n^2)$, because I choose $c = 3$ and $3n^2
   \geq 2n^2 + 100n + 6$ when $n$ is big enough.

*** Big Oh Classes

    Big Oh groups functions into a set of classes, such that all the functions
    in a particular class are equivalent with respect to the Big Oh. A small
    variety of time complexities suffice and account for most algorithms that
    are widely used in practice.

    | Class Name  | Function        |
    |-------------+-----------------|
    | Constant    | $f(n) = 1$      |
    | Logarithmic | $f(n) = \log n$ |
    | Linear      | $f(n) = n$      |
    | Superlinear | $f(n) = n lg n$ |
    | Quadratic   | $f(n) = n^2$    |
    | Cubic       | $f(n) = n^3$    |
    | Exponential | $f(n) = c^n$    |
    | Factorial   | $f(n) = n!$     |

    We say that a faster-growing function *dominates* a slower-growing
    one. Specifically, when $f$ and $g$ belong to different classes (i.e. $f(n)
    \neq \Theta(g(n))$), we say $g$ dominates $f$ when $f(n) = O(g(n))$,
    sometimes written $g >> f$.

*** Big Oh Operations

    The sum of two functions is governed by the dominant one.

    \begin{equation}
    O(f(n)) + O(g(n)) \rightarrow O(max(f(n), g(n)))
    \end{equation}

    Multiplying a function by a constant can not affect its asymptotic
    behavior.

    \begin{equation}
    O(c \cdot f(n)) \rightarrow O(f(n))
    \end{equation}

    When two functions in a product are increasing, both are important.

    \begin{equation}
    O(f(n)) * O(g(n)) \rightarrow O(f(n) * g(n))
    \end{equation}

** Logarithms

   A *logarithm* is simply an inverse exponential function. Saying $b^x = y$ is
   equivalent to saying that $x = \log_b y$. Exponential functions grow at a
   distressingly fast rate. Thus, inverse exponential functions -
   i.e. logarithms - grow refreshingly slowly. Logarithms arise in any process
   where things are repeatedly halved.

   *Binary search* is a good example of an $O(\log n)$ algorithm. If searching
   for a particular name $p$ in a telephone book, we start by comparing $p$
   against the middle. Then we discard half the names. Only twenty comparisons
   suffice to find any name in the million-name Manhattan phone book!

   Logarithms appear in trees (height is $\log_2 n$), bits ($\log_2 n$ bits
   required to store a number in binary).

*** Logarithm Properties

    The $b$ term in $\log_b y$ is the *base* of the logarithm. Three bases are of
    importance for mathematical and historical reasons.

    - Base $b = 2$: The *binary logarithm*, usually denoted $lg n$, is a base 2
      logarithm. Most algorithm applications of logarithms imply binary
      logarithms.
    - Base $b = e$: The *natural log*, usually denoted $ln x$, is a base $e =
      2.71828...$ logarithm.
    - Base $b = 10$: Less common today is the base-10 or *common logarithm*,
      usually denoted as $\log x$.

    \begin{equation}
    \log_x(xy) = \log_a(x) + \log_a(y)
    \end{equation}

    It is easy to convert a logarithm from one base to another. This is a
    consequence of the formula:

    \begin{equation}
    \log_a b = \frac{\log_c b}{\log_c a}
    \end{equation}

    Thus, changing the base of $\log b$ from base-a to base-c simply involves
    dividing by $\log_c a$.

    The base of the logarithm has no real impact on the growth rate. We are
    usually justified in ignoring the base of the logarithm when analyzing
    algorithms.

* Chapter 3: Data Structures

  Classes of *abstract data types* such as containers, dictionaries, and
  priority queues, have many different but functionally equivalent *data
  structures* that implement them. These different data structures realize
  different tradeoffs in the time to execute various operations.

** Contiguous vs. Linked Data Structures

   Data structures are either *contiguous* or *linked*, depending upon whether
   they are based on arrays or pointers.

*** Arrays

    The *array* is the fundamental contiguously-allocated data structures. These
    single slabs of memory have constant access given the index and space
    efficiency. *Dynamic arrays* enable resizing. First, an initial size is
    allocated. If we run out of space, a larger array (usually 2x) is allocated
    and the elements are copied over. Insertion amortizes to $O(1)$.

*** Lists

    The *list* is the simplest linked structure. Each node in the list has data
    and pointer fields. *Pointers* are the connections that hold the pieces of
    together. Pointers represent the address of a location in memory. List don't
    incur overflow, but require extra space for pointer fields and don't given
    efficient random access to items.

** Containers: Stacks and Queues

   A *container* denotes a data structure that permits storage and retrieval of
   data items independent of content. Containers are distinguished by the
   particular retrieval order they support. *Stacks* support retrieval by
   last-in, first-out (LIFO) order. The /put/ and /get/ operations for stacks
   are usually called /push/ and /pop/. *Queues* support retrieval in first in,
   first out (FIFO) order. The /put/ and /get/ operations for queues are usually
   called /enqueue/ and /dequeue/.

** Dictionaries

   The *dictionary* data type permits access to data items by
   content. Operations include /search/ (when given a /key/), /insert/, and
   /delete/.

** Binary Search Trees

   A *binary tree* is recursively defined as being empty or consisting of a root
   node with left and right subtrees. A *binary /search/ tree* labels each node
   in a binary tree with a single key such that for any node labeled $x$, all
   nodes in the left subtree have $keys < x$ while all nodes in the right
   subtree have $keys > x$. Binary tree nodes have left and right point fields,
   an optional parent pointer, and a data field.

*** Traversal

    Traversal involves visiting all nodes. *In-order* traversal of a binary
    search tree can be done recursively with the following.

    #+BEGIN_SRC python
      def traverse(tree):
          if tree:
              traverse(tree.left)
              process(tree.item)
              traverse(tree.right)
    #+END_SRC

    Changing the position of ~process~ gives alternate traversal
    orders. Processing the item first yields a *pre-order* traversal, while
    processing it last gives a *post-order* traversal.

*** Performance

    /Search/, /insert/, and /delete/ all take $O(h)$ time, where $h$ is the
    height of the tree. A perfectly balanced tree has $h = \lceil \log n
    \rceil$. Unfortunately, inserting keys in sorted order produces a skinny
    linear height tree, $h = n$. Randomizing insert order will produce $O(\log
    n)$ height on average.

*** Balanced Search Trees

    *Balanced binary search tree* data structures adjust the tree during
    insertion/delete to guarantee that height will always be $O(\log
    n)$. Balanced tree implementations include *red-black trees* and *splay
    trees*.

** Priority Queues

   *Priority queues* are data structures that provide more flexibility than
   simple sorting, because they allow new elements to enter a system at
   arbitrary intervals. The basic priority queue supports three primary
   operations: /insert/, /find-minimum/maximum/, and
   /delete-minimum/maximum/. Priority queues can be implemented with arrays or
   BSTs, but a particularly nice implementation is the *heap*.

** Hashing and Strings

   *Hash tables* are a very practical way to maintain a dictionary. A *hash
   function* is a mathematical function that maps keys to integers. Hash table
   use the value of a hash function as an index into an array, and store our
   item at that position.

   The first step of the hash function is usually to map each key to a big
   integer. Let $\alpha$ be the size of the alphabet on which a given string $S$
   is written. Let ~char(c)~ be a function that maps each symbol of the alphabet
   to a unique integer from 0 to $\alpha - 1$. The function

   \begin{equation}
   H(S) = \sum_{i = 0}^{|S| - 1} \alpha^{|S| - (i + 1)} \times char(s_i)
   \end{equation}

   maps each string to a unique (but large) integer by treating the characters
   of the strings as "digits" in a base-$\alpha$ number system.

*** Collision Resolution

    Two distinct keys will occasionally hash to the same value. This is a
    *collision*. *Chaining* is the easiest approach to collision
    resolution. Represent the hash table as an array of $m$ linked
    lists. Chaining devotes a considerable amount of memory to pointers. *Open
    addressing* is an alternative to chaining. The hash table is maintained as
    an array of elements, each initialized to null. On an insertion, we check to
    see if the desired position is empty. If so, we insert it. If not, we must
    find some other place to insert it instead. The simplest possibility (called
    *sequential probing*) inserts the item in the next open spot in the table.

*** String Matching via Hashing

    The *Rabin-Karp algorithm* gives a linear-time solution to substring
    search. Substring search asks if string $t$ contains the pattern $p$ as a
    substring, and if so where. In the Rabin-Karp algorithm, We compute a given
    hash function on both the pattern string $p$ and the $|p|$-character
    substring starting from the $i$th position of $t$. If these two strings are
    identical, clearly the resulting hash values must be the same. If the two
    strings are different, the hash values will /almost certainly be different/
    (we can check).

*** Duplicate Detection via Hashing

    The key idea of hashing is to represent a large object using a single
    number. Hashing can be applied to duplicate detection. Suppose we're looking
    to find if a given document is contained in a corpus. Explicitly comparing
    the new document $D$ to all $n$ documents is hopelessly inefficient. But we
    can hash $D$ to an integer, and compare it to the hash codes of the rest of
    the corpus.
