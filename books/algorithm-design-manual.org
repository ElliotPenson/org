#+TITLE: The Algorithm Design Manual
#+AUTHOR: Elliot Penson

This document houses my notes from /The Algorithm Design Manual/ by Steven
S. Skiena.

* Chapter 1: Introduction

  See [[file:~/org/theory/algorithms.org][algorithms]], [[file:~/org/theory/correctness.org][correctness]], and [[file:~/org/theory/program-modeling.org][program modeling]].

* Chapter 2: Algorithm Analysis

  See [[file:~/org/theory/algorithm-analysis.org][algorithm analysis]].

* Chapter 3: Data Structures

  See [[file:../theory/data-structures.org][data structures]], [[file:../theory/bags-stacks-queues.org][bags, stacks, queues]], [[file:../theory/hash-tables.org][hash tables]], and [[file:../theory/trees.org][trees]].

* Chapter 4: Sorting and Searching

  See [[file:../theory/sorting-algorithms.org][sorting algorithms]], [[file:../theory/binary-search][binary search]], and [[file:../theory/divide-and-conquer.org][divide-and-conquer]].

* Chapter 5: Graph Traversal

  A *graph* $G = (V, E)$ consists of a set of *vertices* $V$ together with a set
  $E$ of vertex pairs or *edges*. Graphs can represent essentially /any/
  relationship. The key to using graph algorithms effectively in applications
  lies in correctly modeling your problem so you can take advantage of existing
  algorithms.

** Flavors of Graphs

   Several fundamental properties of graphs impact the choice of the data
   structures used to represent them and algorithms available to analyze them.

   - *Undirected* vs. *Directed*
     - A graph $G = (V, E)$ is undirected if edge $(x, y) \in E$ implies that
       $(y, x) \in E$. If not, we say that the graph is directed.
   - *Weighted* vs. *Unweighted*
     - Each edge (or vertex) in a weighted graph $G$ is assigned a numerical
       value, or weight. In unweighted graphs, there is no cost distinction
       between various edges and vertices.
     - The difference between weighted and unweighted graphs becomes
       particularly apparent in finding the shortest path between two vertices.
   - *Simple* vs. *Non-simple*
     - Any graph that avoids *self-loops* and *multiedges* is called simple. A
       self-loop is an edge $(x, x)$ involving only one vertex. An edge $(x, y)$
       is a multiedge if it occurs more than once in the graph.
   - *Sparse* vs. *Dense*
     - Graphs are sparse when only a small fraction of the possible vertex pairs
       actually have edges defined between them. Graphs where a large fraction
       of the vertex pairs define edges are called dense.
     - The *degree* of a vertex is the number of edges adjacent to it.
     - In a *regular graph*, each vertex has exactly the same degree.
   - *Cyclic* vs. *Acyclic*
     - An acyclic graph does not contain any cycles.
     - *Trees* are connected, acyclic undirected graphs.
   - *Embedded* vs. *Topological*
     - A graph is embedded if the vertices and edges are assigned geometric
       positions.
   - *Implicit* vs. *Explicit*
     - Certain graphs are not explicitly constructed and then traversed, but
       built as we use them.
   - *Labeled* vs. *Unlabeled*
     - Each vertex is assigned a unique name or identifier in a labeled graph to
       distinguish it from all other vertices. In unlabeled graphs, no such
       distinctions have been made.

   *Social networks* are graphs where the vertices are people, and there is an
   edge between two people if and only if they are friends.

** Data Structures for Graphs

   The two basic graph data structures are *adjacency matrices* and *adjacency
   lists*. We assume a graph $G = (V, E)$ contains $n$ vertices and $m$ edges.

   [[file:../images/graph-data-structures.png]]

   Adjacency lists are the right data structure for most applications of
   graphs.

*** Adjacency Matrix

    We can represent $G$ using an $n x n$ matrix $M$, where element $M[i,j] = 1$
    if $(i, j)$ is an edge of $G$, and 0 if it isn't. This allows fast answers
    to the question "is $(i, j)$ in $G$?", and rapid updates for edge insertion
    and deletion. IT may use excessive space for graphs with many vertices and
    relatively few edges, however.

*** Adjacency Lists

    We can more efficiently represent sparse graphs by using linked lists to
    store the neighbors adjacent to each vertex. Adjacency lists make it harder
    to verify whether a given edge $(i, j)$ is in $G$, since we must search
    through th3e appropriate list to find the edge.

** Traversing a Graph

   The key idea behind graph traversal is to mark each vertex when we first
   visit it and keep track of what we have not yet completely explored. Each
   vertex may be /undiscovered/, /discovered/, or /processed/. We must maintain
   a structure containing the vertices that we have discovered but not yet
   completely processed.

*** Breadth-First Search

    The basic breadth-first search algorithm is given below. It takes $O(n + m)$
    time.

    #+BEGIN_SRC python
      def bfs(graph, root):
          discovered = {root}
          parent = {}
          queue = [root]
          while queue:
              current = queue.pop(0)
              for neighbor in graph.find_adjacent(current):
                  if neighbor not in discovered:
                      discovered.add(neighbor)
                      parent[neighbor] = current
                      queue.append(neighbor)
    #+END_SRC

    This implementation of breadth-first search, we assign a direction to each
    edge, from the discoverer ~current~ to the discovered ~neighbor~. We
    maintain a ~parent~ map which defines a tree on the vertices of the
    graph. This tree contains the shortest path from the root to every other
    node in the tree. A breadth-first search tree can be seen in the right of
    the image below.

    [[file:../images/bfs.png]]

    The graph edges that do not appear in the breadth-first search tree also
    have special properties. For undirected graphs, non-tree edges can point
    only to vertices on the same level as the parent vertex, or to vertices on
    the level directly below the parent. These properties follow easily from the
    fact that each path in the tree must be the shortest path in the graph.

**** Applications of Breadth-First Search

***** Connected Components

      A *connected component* of an undirected graph is a maximal set of vertices
      such that there is a path between every pair of vertices. The components
      are separate "pieces" of the graph such that there is no connection between
      the pieces. An amazing number of seemingly complicated problems reduce to
      finding or counting connected components. For example, testing whether a
      puzzle such as the Rubik's cube or the 15 puzzle can be solved from any
      position is really asking whether the graph of legal configurations is
      connected.

      Connected components can be found using breadth-first search since the
      vertex order does not matter. We start from the first vertex. Anything we
      discover during this search must be part of the same connected
      component. We then repeat the search from any undiscovered vertex (if one
      exists) to define the next component, and so on until all vertices have
      been found.

***** Two-Coloring Graphs

      The *vertex-coloring* problem seeks to assign a label (or color) to each
      vertex of a graph such that no edge links any two vertices of the same
      color. We can easily avoid all conflicts by assigning each vertex a unique
      color. However, the goal is to use as few colors as possible.

      A graph is *bipartite* if it can be colored without conflicts while using
      only two colors. Consider the "had-sex-with" graph in a heterosexual
      work. Men have sex only with women, and vice versa. Thus gender defines a
      legal two-coloring, in this simple model.

      We can argument breadth-first search so that whenever we discover a new
      vertex, we color it the opposite of its parent. We check whether any
      nondiscovery edge links two vertices of the same color. Such a conflict
      means that the graph cannot be two-colored.

*** Depth-First Search

    The difference between BFS and DFS results is in the order in which they
    explore vertices. This order depends completely upon the container data
    structure used to store the unprocessed vertices: BFS uses a queue, DFS uses
    a stack. DFS implementations often use recursion instead of an explicit
    stack.

    #+BEGIN_SRC python
     discovered = set()
     time = 0
     entry_time = {}
     exit_time = {}
     parent = {}

     def dfs(root, graph):
         discovered.add(root)
         time += 1
         entry_time[root] = time
         for neighbor in graph.get_adjacent(root):
             if neighbor not in discovered:
                 parent[neighbor] = root
                 dfs(neighbor, graph)
         exit_time[root] = time
         time += 1
    #+END_SRC

    This implementation of depth-first search maintains the traversal time for
    each vertex. The ~time~ clock ticks each time we enter or exit any
    vertex. The time intervals can tell us a vertex's ancestor and how many
    descendants it has.

    [[file:../images/dfs.png]]

    Depth-first search partitions the edges of an undirected graph into exactly
    two classes: *tree edges* and *back edges*. The tree edges discover new
    vertices, and are those encoding in the ~parent~ relation (seen in the image
    above). Back edges are those whose other endpoint is an ancestor of the
    vertex being expanded, so they point back into the tree.

**** Applications of Depth-First Search

***** Finding Cycles

      Back edges are the key to finding a cycle in an undirected graph. If there
      is no back edge, all edges are tree edges, and no cycle exists in a
      tree. But /any/ back edge going from $x$ to an ancestor $y$ creates a
      cycle with the tree path from $y$ to $x$.

***** Articulation Vertices

      [[file:../images/articulation-vertex.png]]

      An *articulation vertex* is a single vertex whose deletion disconnects a
      connected component of the graph. Any graph that contains an articulation
      vertex is inherently fragile, because deleting that single vertex causes a
      loss of connectivity between other nodes. The *connectivity* of a graph is
      the smallest number of vertices whose deletion will disconnect the
      graph. The connectivity is one if the graph has an articulation
      vertex. More robust graphs without such a vertex are said to be
      *biconnected*.

      Testing for articulation vertices by brute force is easy. Temporarily
      delete each vertex $v$, and then do a BFS or DFS traversal of the
      remaining graph to establish whether it is still connected. The total time
      is $O(n(m + n))$.

      DFS gives a clever, linear-time algorithm. Look at the depth-first search
      tree. If this tree represents the entirety of the graph, all internal
      (non-leaf) nodes would be articulation vertices, since deleting any one of
      them would separate a leaf from the root. A depth-first search of a
      /general/ graph partitions the edges into tree edges and back edges. Think
      of these back edges as security cables linking a vertex back to one of its
      ancestors. Finding articulation vertices requires maintaining the extent
      to which back edges (i.e. security cables) link chunks of the DFS tree
      back to ancestor nodes.

**** DFS on Directed Graphs

     When traversing undirected graphs, every edge is either in the depth-first
     search tree or a back edge to an ancestor in the tree. For directed graphs,
     depth-first search labelings can take on a wider range of possibilities:
     tree edges, forward edges, back edges, and cross edges.

***** Topological Sorting

      Topological sorting is the most important operation on directed acyclic
      graphs (DAGs). It orders the vertices on a line such that all directed
      edges go from left to right. Such an ordering cannot exist if the graph
      contains a directed cycle, because there is no way you can keep going
      right on a line and still return back to where you started from!

      Each DAG has at least one topological sort. The importance of topological
      sorting is that it gives us an ordering to process each vertex before any
      of its successors. For example, suppose college courses are vertices and
      prerequisites are edges. Your transcript is a topological sort of
      courses.

      Topological sorting can be performed efficiently using depth-first
      searching. A directed graph is a DAG if and only if no back edges are
      encountered. Labeling the vertices in the reverse order that they are
      marked /processed/ finds a topological sort of a DAG (i.e. record when you
      finish processing then reverse the collection).

* Chapter 6: Weighted Graph Algorithms

  There is an alternate universe of problems for *weighted graphs*. If we're
  traveling to California, we don't just care about the number of roads traveled
  on. The edges of road networks are naturally bound to numerical values such as
  construction cost, traversal time, length, or speed limit. Identifying the
  shortest path in such graphs proves more complicated than breadth-first search
  in unweighted graphs, but opens the door to a wide range of applications.

** Minimum Spanning Trees

   A *spanning tree* of a graph $G = (V, E)$ is a subset of edges from $E$
   forming a tree connecting all vertices of $V$. For edge-weighted graphs, we
   are particularly interested in the *minimum spanning tree* - the spanning
   tree whose sum of edge weights is as small as possible. Minimum spanning tree
   are the answer whenever we need to connect a set of points (representing
   cities, homes, junctions, or other locations) by the smallest amount of
   roadway, wire, or pipe. Minimum spanning trees are also useful for
   clustering.

   There can be more than one minimum spanning tree in a graph. Indeed, all
   spanning trees of an unweighted (or equally weighted) graph $G$ are minimum
   spanning trees, since each contains exactly $n - 1$ equal-weight edges. Such
   a spanning tree can be found using depth-first or breadth-first
   search. Finding a minimum spanning tree is more difficult for general
   weighted graphs, however two different algorithms are presented below.

*** Prim's Algorithm

    *Prim's algorithm* for minimum spanning tree starts from one vertex and
    grows the rest of the tree one edge at a time until all vertices are
    included. *Greedy algorithms* make the decision of what to do next by
    selecting the best local option from all available choices without regard to
    the global structure. Since we seek the tree of minimum weight, the natural
    greedy algorithm for a minimum spanning tree repeatedly selects the smallest
    weight edge that will enlarge the number of vertices in the tree. Prim's
    algorithm can be implemented as $O(m + n\lg{n})$ with a priority-queue.

    #+BEGIN_SRC
    prim-mst(G)
        select an arbitrary vertex s to start the tree from
        while (there are still non tree vertices)
            select the edge of minimum weight between a tree and nontree vertex
            add the selected edge and vertex to the tree T_prim
    #+END_SRC

    The correctness of this algorithm can be proven by contradiction. We assert
    that there must be a specific instant where the tree went wrong. However,
    since we always select the smallest edge, it's not possible for a smaller
    edge to exist than the one we're adding (otherwise it would have already
    been chosen).

*** Kruskal's Algorithm

    *Kruskal's algorithm* is an alternate approach to finding minimum spanning
    trees that proves more efficient on spare graphs. Like Prim's, Kruskal's
    algorithm is greedy. Unlike Prim's, it does not start with a particular
    vertex. Kruskal's algorithm builds up connected components of vertices,
    culminating in a minimum spanning tree. Initially, each vertex forms its own
    separate component in the tree-to-be. The algorithm repeatedly considers the
    lightest remaining edge and tests whether its two endpoints lie within the
    same connected component. If so, this edge will be discarded, because adding
    it would create a cycle in the tree-to-be. If the endpoints are in different
    components, we insert the edge and merge the two components into one. Since
    each connected component is always a tree, we need never explicitly test for
    cycles.

    #+BEGIN_SRC
    kruskal-mst(G)
        put the edges in a priority queue ordered by weight
        count = 0
        while (count < n - 1) do
            get next edge (v, w)
            if (component(v) != component(w))
                add to T_kruskal
                merge component(v) and component(w)
    #+END_SRC

    The speed of Kruskal's algorithm depends on the component test. This test
    may be implemented by a breadth-first or depth-first search in a sparse
    graph. With this approach, Kruskal's algorithm is $O(mn)$. However, a faster
    implementation exists with the *union-find* data structure.

**** The Union-Find Data Structure

     A *set partition* is a partitioning of the elements of some universal set
     (say the integers 1 to $n$) into a collection of disjointed subsets. Thus,
     each element must be in exactly one subset. Set partitions naturally arise
     in graph problems such as connected components (each vertex is in exactly
     one connected component) and vertex coloring (a person may be male or
     female, but not both or neither).

     The connected components in a graph can be represented as a set
     partition. For Kruskal's algorithm to run efficiently, we need a data
     structure that efficiently supports the following operations:

     - $same component(v_1, v_2)$
     - $merge components(C_1, C_2)$

     The union-find data structure represents each subset as a "backwards" tree,
     with pointers from a node to its parent. Each node of this tree contains a
     set element, and the /name/ of the set is taken from the key at the root.

     [[file:../images/union-find.png]]

     We implement our desired component operations in terms of two simpler
     operations, *union* and *find*:

     - $find(i)$
       - Find the root of tree containing element $i$, by walking up the
         pointers until there is nowhere to go. Return the label of the root.
     - $union(i, j)$
       - Link the root of one of the trees (say containing $i$) to the root of
         the tree containing the other (say $j$) so $find(i)$ now equals
         $find(j)$.

     Tree structures can be very unbalanced, so we must limit the height of our
     trees. The most obvious means of control is the decision of which of the
     two component roots becomes the root of the combined component on each
     $union$. To minimize the tree height, it is of course better to make the
     smaller tree the subtree of the bigger one.

     With union-set, we can do both unions and finds in $O(\log{n})$.

** Shortest Paths

   A *path* is a sequence of edges connecting two vertices. The *shortest path*
   from $s$ to $t$ in an unweighted graph can be constructed using a
   breadth-first search from $s$. The minimum-link path is recorded in the
   breadth-first search tree, and it provides the shortest path when all edges
   have equal weight. However, BFS does not suffice to find shortest paths in
   weighted graphs. The shortest weighted path might use a large number of
   edges.

   Finding the shortest path between two nodes in a graph arises in many
   different applications. These may include transportation problems and
   computer network communication problems. Many applications reduce to finding
   shortest path, learn to smell this! Page 212 of The Algorithm Design Manual
   contains a lovely example (/Dialing for Documents/).

*** Dijkstra's Algorithm

    *Dijkstra's algorithm* is the method of choice for finding shortest paths in
    an edge-and/or vertex-weighted graph. Given a particular start vertex $s$,
    it finds the shortest path from $s$ to every other vertex in the graph,
    including your desired destination $t$.

    Suppose the shortest path from $s$ to $t$ in graph $G$ passes through a
    particular intermediate vertex $x$. Clearly, this path must contain the
    shortest path from $s$ to $x$ as its prefix, because if not, we could
    shorten our $s$-to-$t$ path by using a shorter $s$-to-$t$ prefix. Thus, we
    must compute the shortest path from $s$ to $x$ before we find the path from
    $s$ to $t$.

    Dijkstra's algorithm proceeds in a series of rounds, where each round
    establishes the shortest path from $s$ to some new vertex. Specifically, $x$
    is the vertex that minimizes $dist(s, v_i) + w(v_i, x)$ over all finished $1
    \leq i \leq n$, where $w(i, j)$ is the length of the edge from $i$ to $j$,
    and $dist(i, j)$ is the length of the shortest path between them.

    #+BEGIN_SRC python
      import math

      def dijkstra(graph, s, t):
          known = {s}
          distances = {vertex: math.inf for vertex in graph.all_vertices()}
          for neighbor in s.get_neighbors():
              distances[neighbor] = weight(s, neighbor)
          last = s
          while last != t:
              v_next = min(distances[v] for v in (graph.all_vertices() - known))
              for neighbor in v_next.get_neighbors():
                  distances[neighbor] = min(distances[neighbor],
                                            distances[v_next] + weight(v_next, neighbor))
              last = v_next
              known.add(v_next)
    #+END_SRC

    The basic idea is very similar to Prim's algorithm. In each iteration, we
    add exactly one vertex to the tree of vertices for which we /know/ the
    shortest path from $s$. The difference between Dijkstra's and Prim's
    algorithms is how they rate the desirability of each outside vertex. In the
    minimum spanning tree problem, all we cared about was the weight of the next
    potential tree edge. In shortest path, we want to include the closest
    outside vertex (in shortest-path distance) to $s$. This is a function of
    both the new edge weight /and/ the distance from $s$ to the tree vertex it
    is adjacent to.

*** All-Pairs Shortest Path

    Sometimes we want to find the shortest path between all pairs of vertices in
    a given graph. We could solve the *all-pairs shortest path* by calling
    Dijkstra's algorithm from each of the $n$ possible starting vertices
    ($O(n^3)$). But Floyd's all-pairs shortest-path algorithm is a slick way to
    construct an $n x n$ distance matrix from the original weight matrix of the
    graph. This algorithm is also $O(n^3)$, but the loops are so tight and the
    program so short that it runs better in practice.

    Floyd's algorithm starts with the adjacency matrix. The edge $(i, j)$ should
    have its weight in matrix[i][j]. Cells for which the edge doesn't exist
    should be set to MAXINT.

    #+BEGIN_SRC python
      def floyd(adjacency_matrix):
          n_vertices = len(adjacency_matrix)
          for k in range(n_vertices):
              for i in range(n_vertices):
                  for j in range(n_vertices):
                      through_k = adjacency_matrix[x][k] + adjacency_matrix[k][y]
                      if (through_k < adjacency_matrix[x][y]):
                          adjacency_matrix[x][y] = through_k
    #+END_SRC

    We define $W[i, j]^k$ to be the length of the shortest path from $i$ to $j$
    using only vertices numbered from 1, 2, ..., $k$ as possible intermediate
    vertices. At each iteration, we allow a richer set of possible shortest
    paths by adding a new vertex as a possible intermediary. Allowing the $k$th
    vertex as a stop helps only if there is a short path that goes through $k$,
    so $W[i, j]^k = min(W[i, j]^{k - 1}, W[i, k]^{k - 1},  + W[k, j]^{k - 1})$.

* Chapter 7: Combinatorial Search and Heuristic Methods

  See [[file:../theory/combinatorial-search-and-heuristic-methods.org][combinatorial search and heuristic methods]].

* Chapter 8: Dynamic Programming

  Dynamic programming is a technique for efficiently implementing a recursive
  algorithm by storing partial results. Dynamic programming guarantees
  /correctness/ by searching all possibilities and provides /efficiency/ by
  storing results to avoid recomputing. If the naive recursive algorithm
  computes the same subproblems over and over again, storing the answer for each
  subproblem in a table to look up instead of recompute can lead to an efficient
  algorithm. Dynamic programming is essentially a tradeoff of space for time.

** Fibonacci Example

   Let's look at a simple program for computing the /n/th Fibonacci number.

   #+BEGIN_SRC python
     def fib(n):
         if n == 0:
             return 0
         if n == 1:
             return 1
         return fib(n - 1) + fib(n - 2)
   #+END_SRC

   The course of execution for this recursive algorithm is illustrated by its
   *recursion tree*.

   [[file:../images/fib-recursion-tree.png]]

   Note that $F(4)$ is computed on both sides of the recursion tree, and $F(2)$
   is computed no less than five times in this small example. This redundancy
   drastically affects performance.

   We can improve performance by storing (or *caching*) the results of each
   Fibonacci computation $F(k)$ indexed by the parameter $k$.

   #+BEGIN_SRC python
     cache = {0: 0, 1: 1}
     def fib(n):
         if n not in cache:
             cache[n] = fib(n - 1) + fib(n - 2)
         return cache[n]
   #+END_SRC

   This approach is a simple way to get /most/ of the benefits of full dynamic
   programming. Here's the recursion tree:

   [[file:../images/fib-caching.png]]

   Let's go a step further with full dynamic programming! We can calculate
   $F(n)$ in linear time and space with no recursive calls by explicitly
   specifying the order of evaluation of the recurrence relation.

   #+BEGIN_SRC python
     def fib(n):
         f = [0, 1]
         for i in range(2, n + 1):
             f.append(f[i - 1] + f[i - 2])
         return f[n]
   #+END_SRC

   However, more careful study shows that we do not need to store all the
   intermediate values for the entire period of execution.

   #+BEGIN_SRC python
     def fib(n):
         if n == 0:
             return 0

         back_2, back_1 = 0, 1
         for _ in range(2, n):
             back_2, back_1 = back_1, back_1 + back_2
         return back_1 + back_2
   #+END_SRC

   This analysis reduces the storage demands to constant space with no
   asymptotic degradation in running time.

** Approximate String Matching

   To deal with inexact string matching, we must first define a cost function
   telling us how far apart two strings are - i.e., a distance measure between
   pairs of strings. *Edit distance* reflects the number of /changes/ that must
   be made to convert one string to another. There are three natural types of
   changes: /substitution/, /insertion/, and /deletion/. Edit distance assigns
   each operation an equal cost of 1. Here's a recursive edit distance function:

   #+BEGIN_SRC python
     def edit_distance(source, target):
         if not source:
             return len(target)
         if not target:
             return len(source)

         substitution_cost = 0 if source[-1] == target[-1] else 1
         return min(edit_distance(source[:-1], target[:-1]) + substitution_cost,
                    edit_distance(source, target[:-1]) + 1,  # insertion
                    edit_distance(source[:-1], target) + 1)  # deletion
   #+END_SRC

   This program is absolutely correct but impossible slow. A table-based,
   dynamic programming implementation of this algorithm is given below. ~costs~
   is a two-dimensional matrix where each cell contains the optimal solution to
   a subproblem (i.e. ~costs[x][y]~ is ~edit_distance(source[:x],
   target[:y])~).

   #+BEGIN_SRC python
     def edit_distance(source, target):
         costs = [[None for _ in range(len(target) + 1)]
                  for _ in range(len(source) + 1)]

         for index in range(len(costs)):
             costs[index][0] = index
         for index in range(len(costs[0])):
             costs[0][index] = index

         for x in range(1, len(source) + 1):
             for y in range(1, len(target) + 1):
                 substitution_cost = 0 if source[x - 1] == target[y - 1] else 1
                 costs[x][y] = min(costs[x - 1][y - 1] + substitution_cost,
                                   costs[x - 1][y] + 1,  # insertion
                                   costs[x][y - 1] + 1)  # deletion
         return costs[-1][-1]
   #+END_SRC

   The first row and the first column represent the empty prefix of the source
   and target, respectively. This is why the matrix height/width is larger than
   the source/target length.

   Note that it is unnecessary to store the entire ~O(mn)~ matrix. The
   recurrence only requires two rows at a time. Thus, this algorithm could be
   further optimized to ~O(n)~ space without changing the time complexity.

** Dynamic Programming in Practice

   There are three steps involved in solving a problem by dynamic programming:

   1. Formulate the answer as a recurrence relation or recursive algorithm.
   2. Show that the number of different parameter values taken on by your
      recurrence is bounded by a (hopefully small) polynomial.
   3. Specify an order of evaluation for the recurrence so the partial results
      you need are always available when you need them.

   In practice, you'll find that dynamic programming algorithms are usually
   easier to work out from scratch than look up.

** The Partition Problem

   Suppose three workers are given the task of scanning through a shelf of books
   in search of a given piece of information. To get the job done fairly and
   efficiently, the books are to be partitioned among the three workers. If the
   books are the same length, the job is easy: ~100 100 100 | 100 100 100 | 100
   100 100~. If the books are not the same length, the task becomes more
   difficult (~100 200 300 400 500 | 600 700 | 800 900~). An algorithm that
   solves this *linear partition problem* takes as input an arrangement $S$ of
   nonnegative numbers and an integer $k$. The algorithm should partition $S$
   into $k$ or fewer ranges, to minimize the maximum sum over all ranges,
   without reordering any of the numbers.

   A heuristic to solve this problem might compute the average size of a
   partition and then try and insert dividers to come close to this
   average. Unfortunately, this method is doomed to fail on certain inputs.

   Instead, consider a recursive, exhaustive search approach to solving this
   problem. The /k/th partition starts right ater we placed the (k - 1)st
   divider. Where can we place this last divider? Between the ith and (i + 1)st
   elements for some $i$, where $1 \leq i \leq n$. What is the cost of this? The
   total cost will be the larger of two qualtities - (1) the cost of the last
   partition and (2) the cost of the largest partition formed to the left of
   $i$. What is the size of this let partition? To minimize our total, we want
   to use the $k - 2$ remaining dividers to partition the elements $\{s_1, ...,
   s_i\}$ as equally as possible. This is a smaller instance of the same problem
   and hence can be solved recursively!

   Therefore, let us define $M[n, k]$ to be the minimum possible cost over all
   partitions of $\{s_1, ..., s_n\}$ into $k$ ranges, where the cost of a
   partition is the largest sum of elements in one of its parts. Thus defined,
   this function cab be evaluated:

   \begin{equation}
   M[n,k] = min(i=1, n)(max(M[i, k - 1], \sum_{j = i + 1}^{n} s_j))
   \end{equation}

   This recurrence can be solved with dynamic programming in $O(kn^2)$
   time. Note that we also need a second matrix, $D$ to reconstruct the optimal
   partition. Whenever we update the value of $M[i, j]$, we record which divider
   position was required to achieve that value.

** Parsing Context-Free Grammars

   See [[file:../theory/cfg.org]].
